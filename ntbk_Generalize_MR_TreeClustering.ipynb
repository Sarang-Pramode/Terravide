{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Lidar Tilesets and Perform MR classifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.modules.utils as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UTIL] Function - Datayear :  2017\n"
     ]
    }
   ],
   "source": [
    "filenames = util.FTP_GetFileList(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Las File to hard disk\n",
    "#List in Hardisk - 15212, 25192, 47217, 47219, 987210\n",
    "las_fileID = 25192\n",
    "year = 2017\n",
    "\n",
    "Filename = str(las_fileID)+'.las'\n",
    "External_Disk_Path = '/Volumes/Elements/Terravide/Datasets/FTP_files/LiDAR/'\n",
    "util.FTP_download_lasfile(Filename,year,folderpath=External_Disk_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.modules.MultipleReturnsClassification as MRC\n",
    "import time\n",
    "import pptk\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defines (Also in Utils)\n",
    "\n",
    "def View3Dpoints(points, color=[[1,0,0]]):\n",
    "    \"\"\"Calls PPTK with basic config to plot 3d points\n",
    "\n",
    "    Args:\n",
    "        points (Nx3 Numpy Array): NX3 numpy array\n",
    "    \"\"\"\n",
    "    exitViewerFlag = False\n",
    "    while not exitViewerFlag:\n",
    "        v = pptk.viewer(points, color*len(points))\n",
    "        v.set(show_grid=False)\n",
    "        v.set(show_axis=False)\n",
    "        v.set(bg_color = [0,0,0,0])\n",
    "        v.set(point_size = 0.0004)\n",
    "        exitViewerFlag = int(input(\"Enter a 1 to exit viewer : \"))\n",
    "\n",
    "    v.close()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object to handle las preprocessing\n",
    "LasHandling = MRC.LFP\n",
    "\n",
    "#record start time of script\n",
    "script_start = time.time()\n",
    "\n",
    "\n",
    "TileDivision = 12\n",
    "rows, cols = (TileDivision, TileDivision)\n",
    "\n",
    "\n",
    "lasfilepath = External_Disk_Path+'NYC_'+str(year)+'/'+str(las_fileID)+'.las'\n",
    "\n",
    "#Read las file\n",
    "lasfile_object = LasHandling.Read_lasFile(lasfilepath)\n",
    "\n",
    "#Create Dataframe from lasfile\n",
    "lidar_df, rawpoints = LasHandling.Create_lasFileDataframe(lasfileObject=lasfile_object)\n",
    "\n",
    "#Divide lidar_df into smaller portion for development\n",
    "portion_size = 100 # 1-100 %\n",
    "lidar_df = lidar_df.head(int(len(lidar_df)*(portion_size/100)))\n",
    "\n",
    "#sanity check\n",
    "pptk.viewer(lidar_df.iloc[:,:3].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Z limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_Height_Limit = 80 # in m\n",
    "lidar_df_ZLimited  = lidar_df[lidar_df.Z < Tree_Height_Limit]\n",
    "\n",
    "lidar_df = lidar_df_ZLimited\n",
    "pptk.viewer(lidar_df.iloc[:,:3].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract MR and SR points from Dataframe\n",
    "MR_df = LasHandling.Get_MRpoints(lidar_df)\n",
    "SR_df = LasHandling.Get_SRpoints(lidar_df)\n",
    "\n",
    "#lasTile class\n",
    "TileObj_SR = MRC.MR_class(SR_df,TileDivision) #Single Return Points\n",
    "TileObj_MR = MRC.MR_class(MR_df,TileDivision) #Multiple Return Points\n",
    "\n",
    "#Serialized Creation of Lidar Subtiles\n",
    "lidar_TilesubsetArr = TileObj_MR.Get_subtileArray()\n",
    "\n",
    "# type(lidar_TilesubsetArr[0][0]) -> pandas.core.frame.DataFrame\n",
    "# [[0]*self.cols for _ in range(self.rows)]\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# #sanity check\n",
    "# temp_segmentPoints = lidar_TilesubsetArr[1][0].iloc[:,:3].to_numpy()\n",
    "# pptk.viewer(temp_segmentPoints)\n",
    "\n",
    "# #MR points plot\n",
    "# pptk.viewer(MR_df.iloc[:,:3].to_numpy())\n",
    "\n",
    "# #SR points plot\n",
    "# pptk.viewer(SR_df.iloc[:,:3].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Approximate Location of tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Lat , Long\n",
    "ix, iy = np.mean(MR_df.X.to_numpy()), np.mean(MR_df.Y.to_numpy()) \n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(\"epsg:2263\", \"epsg:4326\")\n",
    "lat, lon = transformer.transform(ix*3.28, iy*3.28)\n",
    "print(str(lat)+\",\"+str(lon))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics import silhouette_score # How best can we seperate clusters\n",
    "\n",
    "# from scipy.stats import ks_2samp #Check how different each distributions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "\n",
    "def Get_eps_NN_KneeMethod(cluster_df, N_neighbors = 12, display_plot=False):\n",
    "\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=N_neighbors)\n",
    "    neighbors = nearest_neighbors.fit(cluster_df)\n",
    "    distances, indices = neighbors.kneighbors(cluster_df)\n",
    "    distances = np.sort(distances[:,N_neighbors-1], axis=0)\n",
    "\n",
    "    i = np.arange(len(distances))\n",
    "    knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "    if (display_plot):\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        knee.plot_knee()\n",
    "        plt.xlabel(\"Points\")\n",
    "        plt.ylabel(\"Distance\")\n",
    "        print(distances[knee.knee])\n",
    "    \n",
    "    return distances[knee.knee]\n",
    "\n",
    "# def Get_Optimal_MinSamples(points,ep,start=20,end=70,step=10):\n",
    "\n",
    "#     range_min_samples = np.arange(start, end, step)\n",
    "\n",
    "#     S_score_min_samples_hashmap = {} # S_score -> min_samples\n",
    "\n",
    "#     for m in range_min_samples :\n",
    "                \n",
    "#         db = DBSCAN(eps=ep, min_samples=m).fit(points)\n",
    "#         core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#         core_samples_mask[db.core_sample_indices_] = True\n",
    "#         labels = db.labels_\n",
    "#         #print(set(labels))\n",
    "#         if(len(set(labels))) < 2 :\n",
    "#              return (1.5,0)\n",
    "#         silhouette_avg = silhouette_score(points, labels)\n",
    "#         print(\"For eps value = \",ep,\"; Min Samples = \",m, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "#         if silhouette_avg not in S_score_min_samples_hashmap:\n",
    "#             S_score_min_samples_hashmap[silhouette_avg] = ep\n",
    "        \n",
    "#         Best_min_samples = S_score_min_samples_hashmap[max(S_score_min_samples_hashmap)]\n",
    "    \n",
    "#     return (Best_min_samples, max(S_score_min_samples_hashmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_points(points):\n",
    "    return points / np.linalg.norm(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding optimal min_samples\n",
    "# range_min_samples = np.arange(10, 60, 10)\n",
    "# ep = distances[knee.knee]\n",
    "\n",
    "# points = cluster_df.to_numpy()\n",
    "\n",
    "# for m in range_min_samples :\n",
    "\n",
    "#         print(ep,m)\n",
    "                \n",
    "#         db = DBSCAN(eps=ep, min_samples=m).fit(points)\n",
    "#         core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#         core_samples_mask[db.core_sample_indices_] = True\n",
    "#         labels = db.labels_\n",
    "        \n",
    "#         silhouette_avg = silhouette_score(points, labels)\n",
    "#         print(\"For eps value = \",ep, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot EPS variations on tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_eps = [] #Stores all eps values by tile id\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "\n",
    "        tile_eps = Get_eps_NN_KneeMethod(cluster_df)\n",
    "\n",
    "        All_eps.append(tile_eps)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(All_eps)\n",
    "plt.xlabel(\"Tile ID\")\n",
    "plt.ylabel(\"Eps value\")\n",
    "\n",
    "Optimal_EPS = np.mean(All_eps)\n",
    "print(\"EPS : \",Optimal_EPS)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying only Tree points from Multiple Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tilecounter = 0\n",
    "Trees_Buffer = []\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        print('-'*40)\n",
    "        \n",
    "        print(\"TILE ID : \",Tilecounter)\n",
    "        Tilecounter = Tilecounter + 1\n",
    "\n",
    "        cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "\n",
    "        tile_eps = Get_eps_NN_KneeMethod(cluster_df) #round(Optimal_EPS,2)\n",
    "        print(tile_eps)\n",
    "\n",
    "        tile_segment_points = lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()\n",
    "\n",
    "        subTileTree_Points,  _ = TileObj_MR.Classify_MultipleReturns(tile_segment_points,tile_eps)\n",
    "\n",
    "        for t in subTileTree_Points:\n",
    "            Trees_Buffer.append(t)\n",
    "        \n",
    "        print(\"Trees added\")\n",
    "\n",
    "Trees_Buffer = np.array(Trees_Buffer)\n",
    "\n",
    "#pptk.viewer(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting inlier and outlier\n",
    "p1 = Trees_Buffer\n",
    "p2 = MR_df.iloc[:,:3].to_numpy()\n",
    "All_points_1 = np.concatenate((p1, p2), axis=0)\n",
    "rgb_p2 =  [[1,0,0]]*len(p2) #Set red colour\n",
    "rgb_p1 = [[0,1,0]]*len(p1) #set green colour\n",
    "All_rgb = np.concatenate((rgb_p1, rgb_p2,), axis=0)\n",
    "\n",
    "#Red - Inlier - ground plane , Green - Outlier\n",
    "v = pptk.viewer(All_points_1, All_rgb)\n",
    "v.set(show_grid=False)\n",
    "v.set(show_axis=False)\n",
    "v.set(bg_color = [0,0,0,0])\n",
    "v.set(point_size = 0.04)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Tree Clusters from Multiple Returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Census\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Census\n",
    "\n",
    "path = \"/Volumes/Elements/Terravide/Datasets/NYC_Tree_Dataset/2015StreetTreesCensus_TREES.csv\"\n",
    "Tree_Dataset = pd.read_csv(path)\n",
    "las = laspy.read(lasfilepath) # .las file taken from NYC topbathymetric 2017 Lidar data\n",
    "\n",
    "point_format = las.point_format\n",
    "lidarPoints = np.array((las.X,las.Y,las.Z,las.intensity,las.classification, las.return_number, las.number_of_returns)).transpose()\n",
    "lidar_dfRead = pd.DataFrame(lidarPoints)\n",
    "# correct XYZ scale to state plane\n",
    "lidar_dfRead[0] = lidar_dfRead[0]/100\n",
    "lidar_dfRead[1] = lidar_dfRead[1]/100\n",
    "lidar_dfRead[2] = lidar_dfRead[2]/100\n",
    "# find bounds of las file\n",
    "x_min = lidar_dfRead[0].min()\n",
    "x_max = lidar_dfRead[0].max()\n",
    "y_min = lidar_dfRead[1].min()\n",
    "y_max = lidar_dfRead[1].max()\n",
    "# select trees in lidar footprint in a new dataframe\n",
    "trees_df2 = Tree_Dataset.copy()\n",
    "trees_df2 = trees_df2[trees_df2['x_sp']>x_min]\n",
    "trees_df2 = trees_df2[trees_df2['x_sp']<x_max]\n",
    "trees_df2 = trees_df2[trees_df2['y_sp']>y_min]\n",
    "trees_reduced_df = trees_df2[trees_df2['y_sp']<y_max]\n",
    "\n",
    "True_TreeLoc = {\"Tree_X\" : trees_reduced_df.x_sp.to_numpy(),\n",
    "                \"Tree_Y\" : trees_reduced_df.y_sp.to_numpy()}\n",
    "\n",
    "# Used to Map Tree ID to Predicted Tree Cluster\n",
    "True_TreeLoc_X = True_TreeLoc['Tree_X']/3.28\n",
    "True_TreeLoc_Y = True_TreeLoc['Tree_Y']/3.28\n",
    "Tree_Census_Loc_xy = np.stack((True_TreeLoc_X,True_TreeLoc_Y),axis=1)\n",
    "\n",
    "#Use KDTree\n",
    "\n",
    "from scipy import spatial\n",
    "Tree_Census_KDTree = spatial.KDTree(Tree_Census_Loc_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tilecounter = 0\n",
    "Trees_Buffer = []\n",
    "\n",
    "#JSON Buffer vars\n",
    "TreeClusterID = 0\n",
    "JSON_data_buffer = {\n",
    "    \"lasFileID\" : las_fileID,\n",
    "    \"RecordedYear\" : year,\n",
    "    \"MR_TreeClusterDict\" : []\n",
    "}\n",
    "\n",
    "for row in range(TileDivision):\n",
    "    for col in range(TileDivision):\n",
    "\n",
    "        print('-'*40)\n",
    "        \n",
    "        print(\"TILE ID : \",Tilecounter)\n",
    "        print(\"TreeCLusterID : \",TreeClusterID)\n",
    "        Tilecounter = Tilecounter + 1\n",
    "\n",
    "        cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "\n",
    "        tile_eps = Get_eps_NN_KneeMethod(cluster_df) #round(Optimal_EPS,2)\n",
    "        print(\"EPS :\",tile_eps)\n",
    "\n",
    "        tile_segment_points = lidar_TilesubsetArr[row][col].iloc[:,:3].to_numpy()\n",
    "\n",
    "\n",
    "        subTileTree_Points, _ , TreeCounterID = TileObj_MR.Get_MultipleReturnTreeCLusters(\n",
    "                                    tile_segment_points,\n",
    "                                    Tree_Census_KDTree,5,trees_reduced_df, #tree mapping tolerance thresh = 5 m\n",
    "                                    Tilecounter,\n",
    "                                    las_fileID,\n",
    "                                    JSON_data_buffer,\n",
    "                                    TreeClusterID,\n",
    "                                    tile_eps)\n",
    "\n",
    "        TreeClusterID = TreeCounterID\n",
    "\n",
    "        for t in subTileTree_Points:\n",
    "            Trees_Buffer.append(t)\n",
    "        \n",
    "        print(\"Trees Clusters added to JSON\")\n",
    "\n",
    "Trees_Buffer = np.array(Trees_Buffer)\n",
    "\n",
    "#pptk.viewer(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "lasfilefoldernName = \"TreeCluster_JSON\"\n",
    "\n",
    "path = \"/Volumes/Elements/TerraVide/Datasets/\" + \"Package_Generated/\" + lasfilefoldernName\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "isExist = os.path.exists(path)\n",
    "\n",
    "if not isExist:\n",
    "  # Create a new directory because it does not exist \n",
    "  os.makedirs(path)\n",
    "\n",
    "with open(path+\"/\"+str(las_fileID)+\"_\"+str(year)+\"_.json\", \"w\") as jsonFile:\n",
    "  jsonFile.truncate(0)\n",
    "  json.dump(JSON_data_buffer, jsonFile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below block maps distribution to hyperparameter - Maybe not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tilecounter = 0\n",
    "# Trees_Buffer = []\n",
    "\n",
    "# # Hashmap to store MinSamples with distribution\n",
    "# MinSamples_Distribution_Hmap = {} # Df_Z -> MinSamples\n",
    "\n",
    "# for row in range(2):\n",
    "#     for col in range(4):\n",
    "\n",
    "#         print('-'*40)\n",
    "        \n",
    "#         print(\"TILE ID : \",Tilecounter)\n",
    "#         Tilecounter = Tilecounter + 1\n",
    "\n",
    "#         cluster_df = lidar_TilesubsetArr[row][col].iloc[:,:3]\n",
    "\n",
    "#         tile_eps = Get_Optimal_Eps(cluster_df)\n",
    "\n",
    "#         tile_segment_points = cluster_df.to_numpy()\n",
    "\n",
    "#         # tile_segment_points_X = lidar_TilesubsetArr[row][col].iloc[:,0].to_numpy()\n",
    "#         # tile_segment_points_Y = lidar_TilesubsetArr[row][col].iloc[:,1].to_numpy()\n",
    "#         # tile_segment_points_Z = lidar_TilesubsetArr[row][col].iloc[:,2].to_numpy()\n",
    "        \n",
    "#         if (len(tile_segment_points) > 0): #buffer has points to classify\n",
    "\n",
    "#             if not MinSamples_Distribution_Hmap: #Dict is empty\n",
    "                \n",
    "#                 #find optimal MinSamples\n",
    "#                 Optimal_MinSamples, S_Score = Get_Optimal_MinSamples(points=tile_segment_points, ep=tile_eps)\n",
    "\n",
    "#                 # Mapping Z distribution -> MinSamples value,  to check KS_statistic later to compare distribution\n",
    "#                 MinSamples_Distribution_Hmap[tuple(tile_segment_points[:,2])] = [Optimal_MinSamples,S_Score]\n",
    "            \n",
    "#             else: #Dict is not empty\n",
    "\n",
    "#                 #Find Minimum KS statistic and check is KS statistic is less than 0.5 (arbitarily chosen)\n",
    "\n",
    "#                 KS_stat_list = [] # Empty list for each tile to compare KS statistic as seen from previous tiles\n",
    "#                 min_KS_stat_info = [100,-1] #[KS_stat, Key of dict after comparison] #default 100,-1\n",
    "\n",
    "#                 for k in MinSamples_Distribution_Hmap.keys() : \n",
    "\n",
    "#                     z1 = Normalize_points(k) # get Z_points from dict\n",
    "#                     z2 = Normalize_points(tile_segment_points[:,2]) # get Z_points from new tile\n",
    "\n",
    "#                     cur_statistic, pvalue = ks_2samp(z1, z2) #compare\n",
    "\n",
    "#                     #Logs\n",
    "#                     # print(f'K-S statistic (z): {cur_statistic:.3f}')\n",
    "#                     # print(f'p-value (z): {pvalue:.3f}')\n",
    "\n",
    "#                     #if pvalue < 0.05: #p < 0.05 , # NOTE: Not sure to include, maybe identical buildings?\n",
    "#                     KS_stat_list.append(cur_statistic) #add to list\n",
    "\n",
    "#                     if cur_statistic < min_KS_stat_info[0]:\n",
    "#                         #update best distribution\n",
    "#                         min_KS_stat_info[0] = cur_statistic\n",
    "#                         min_KS_stat_info[1] = k\n",
    "                \n",
    "#                 print(\"KS Stat : \",min_KS_stat_info[0])\n",
    "\n",
    "#                 #NOTE : #if KS_statistic is significant (high) it represents that the 2 disttribtuions are very Different and likely drawn from different distributions\n",
    "\n",
    "#                 if min_KS_stat_info[0] < 0.6: \n",
    "\n",
    "#                     #get optimal MinSamples from best matching distribution\n",
    "\n",
    "#                     Optimal_MinSamples, S_Score = MinSamples_Distribution_Hmap[min_KS_stat_info[1]] #Get the optimal MinSamples and s_score by passing key (df_z values) of best match (low KS)\n",
    "#                 else:\n",
    "\n",
    "#                     #Recalculate MinSamples and add it to hashmap \n",
    "\n",
    "#                     #find optimal MinSamples\n",
    "#                     Optimal_MinSamples, S_Score = Get_Optimal_MinSamples(tile_segment_points)\n",
    "\n",
    "#                     # Mapping Z distribution -> MinSamples value,  to check KS_statistic later to compare distribution\n",
    "#                     MinSamples_Distribution_Hmap[tuple(tile_segment_points[:,2])] = [Optimal_MinSamples,S_Score]\n",
    "                       \n",
    "#             # Logs\n",
    "#             print(\"OPTIMAL MinSamples : \",Optimal_MinSamples)\n",
    "            \n",
    "\n",
    "#             subTileTree_Points,  _ = TileObj_MR.Classify_MultipleReturns(tile_segment_points, hp_eps=tile_eps, hp_min_points=Optimal_MinSamples)\n",
    "\n",
    "#             for t in subTileTree_Points:\n",
    "#                 Trees_Buffer.append(t)\n",
    "            \n",
    "#             print(\"Trees added\")\n",
    "\n",
    "# Trees_Buffer = np.array(Trees_Buffer)\n",
    "\n",
    "# pptk.viewer(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pptk.viewer(Trees_Buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pptk.viewer(MR_df.iloc[:,:3].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in EPS_Distribution_Hmap.keys() : \n",
    "\n",
    "    z1 = Normalize_points(EPS_Distribution_Hmap[k]) # get Z_points from dict\n",
    "    print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmap_Keys = EPS_Distribution_Hmap.keys()\n",
    "\n",
    "for k in range(len(hmap_Keys)):\n",
    "    print(k)\n",
    "    print(len(hmap_Keys[k]))\n",
    "\n",
    "    z1 = Normalize_points(EPS_Distribution_Hmap[hmap_Keys[k]]) # get Z_points from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Normalize_points(lidar_TilesubsetArr[0][0].iloc[:,:3].to_numpy())\n",
    "t2 = Normalize_points(lidar_TilesubsetArr[0][1].iloc[:,:3].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(t1[:,0], t1[:,1], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "# Extract the x and y coordinates from the point cloud datasets\n",
    "x1 = t1[:,0]\n",
    "y1 = t1[:,1]\n",
    "z1 = t1[:,2]\n",
    "x2 = t2[:,0]\n",
    "y2 = t2[:,1]\n",
    "z2 = t2[:,2]\n",
    "\n",
    "# Perform the K-S test on the x coordinates\n",
    "statistic, pvalue = ks_2samp(x1, x2)\n",
    "print(f'K-S statistic (x): {statistic:.3f}')\n",
    "print(f'p-value (x): {pvalue:.3f}')\n",
    "\n",
    "# Perform the K-S test on the y coordinates\n",
    "statistic, pvalue = ks_2samp(y1, y2)\n",
    "print(f'K-S statistic (y): {statistic:.3f}')\n",
    "print(f'p-value (y): {pvalue:.3f}')\n",
    "\n",
    "\n",
    "#NOTE : Z value varies the most, if KS is large , there are from different distributions\n",
    "# Perform the K-S test on the z coordinates\n",
    "statistic, pvalue = ks_2samp(z1, z2)\n",
    "print(f'K-S statistic (z): {statistic:.3f}')\n",
    "print(f'p-value (z): {pvalue:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z1 = norm1[:,2]\n",
    "z2 = norm2[:,2]\n",
    "\n",
    "# Set up the figure and axes for the histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "# Plot the histogram for the z1 norm1\n",
    "ax1.hist(z1, bins=20)\n",
    "ax1.set_xlabel('z1')\n",
    "ax1.set_ylabel('count')\n",
    "\n",
    "# Plot the histogram for the z2 norm2\n",
    "ax2.hist(z2, bins=20)\n",
    "ax2.set_xlabel('z2')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "395b5501581aedb2d94c1f1944a406c3cb0aeb8faa5df16e5001f1dc5b0910fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
